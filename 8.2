
Inferential statistics II - Bootstrapping
Introduction
In the previous frequentist mini-projects, you did frequentist calculations to perform inference from a sample of data. Such inference relies on theory largely developed from the 19th-Century onwards that is subject to certain assumptions or theoretical limits. These are fine if those assumptions hold for the particular case you're working on, and what you want to do has a known theoretical distribution (for example the mean of a sampling distribution that we looked at in the previous mini-project.)

In this mini-project, you'll use the same medical charge data you used in the frequentist inference mini-project, but this time you'll make inferences about the population using bootstrapping (ie. simulating repeated re-runs of an experiment.) If frequentism is about using assumptions and theoretical results to calculate what we expect to happen were an experiment to be run again and again and again, then bootstrapping is about using computing power to essentially re-run the sample draw again and again and again to see what actually happens.

Prerequisites
While these exercises do not strictly depend on these concepts, we encourage you to complete the previous mini-projects before starting this one so that you can approach this assignment with a good understanding of frequentist concepts like:

the z-statistic
the t-statistic
the difference and relationship between the two
the Central Limit Theorem, its assumptions and consequences
how to estimate the population mean and standard deviation from a sample
the concept of a sampling distribution of a test statistic, particularly for the mean
how to combine these concepts to calculate confidence intervals and p-values
how those confidence intervals and p-values allow you to perform hypothesis (or A/B) tests
To complete mini-project, it's important that you first complete the bootstrap resources listed in this subunit, as they contain valuable information about how to calculate bootstrap replicates of summary statistics. Having an basic understanding of what confidence intervals and p-values are will also be helpful (we touch on them in this mini-project, but please speak to your mentor or conduct individual research if you'd like to learn more.)

In [1]:
import pandas as pd
import numpy as np
from numpy.random import seed
import matplotlib.pyplot as plt
Medical charge data set
In [2]:
med_charges = pd.read_csv('data/insurance2.csv')
In [3]:
med_charges.head()
Out[3]:
age	sex	bmi	children	smoker	region	charges	insuranceclaim
0	19	0	27.900	0	1	3	16884.92400	1
1	18	1	33.770	1	0	2	1725.55230	1
2	28	1	33.000	3	0	2	4449.46200	0
3	33	1	22.705	0	0	1	21984.47061	0
4	32	1	28.880	0	0	1	3866.85520	1
In the previous assignment, you used the frequentist approach to estimate the lower limit for the 95% confidence interval on the mean hospital charge. This approach relies on statistical theory that has been developed over the years and is also limited to statistics for which theoretical results on the sampling distribution exist. These results are remarkably useful and applicable much of the time and under a surprisingly wide range of conditions.

Having calculated the 95% lower confidence interval using frequentist theory in the previous exercise, you'll now use bootstrap inference to verify your calculations and check that you get consistent results without making the assumptions required before. After all, the distribution of charges really was very non-normal.

Q: Use bootstrap sampling to estimate the same 95% confidence interval lower limit as before.

A:Here the lower limit of the 95% confidence interval is 12724.47.

In [7]:
np.random.seed(47)
N_rep = 10000
bs_rep = np.empty(N_rep)
for i in range(N_rep):
    bs_rep[i] = np.mean(np.random.choice(med_charges['charges'], size=len(med_charges['charges'])
In [12]:
np.percentile(bs_rep, [5, 100])
Out[12]:
array([12724.4679217 , 14479.36934788])
If you performed 10000 replicates immediately after setting the random seed to 47, you should get the value 12724 here, which compares very well with the value 12725 obtained using the t-distribution confidence interval previously. It is a most pleasant result to see the predictions of classical frequentist theory match with results that are now possible through the number-crunching ability of computers.

Remember, in the previous mini-projects, we saw that there are two ways of performing a t-test from a sample, depending on whether we can assume the groups have equal variance or not. We can actually easily test this using the bootstrap approach!

Q: Calculate the 95% confidence interval for the difference between the standard deviations of insurance and non-insurance claim charges (insured - non-insured). Calculate the differences over 10000 replicates. Plot the histogram of values and mark the locations of the percentiles. State the null and alternative hypothesis and comment on whether you would retain or reject the null hypothesis in this case and why.

A:95% CI - [6676.06177337, 8486.94648372]. Null Hypothesis: H0 - Diff in std deviation = 0, Alternative Hypothesis : Ha - Diff in std deviation != 0. Looking at the confidence interval and histogram we can straight away reject the Null hypothesis

In [19]:
claimed_insurance = med_charges[med_charges['insuranceclaim'] == 1].charges
notclaimed_insurance = med_charges[med_charges['insuranceclaim'] == 0].charges
In [20]:
bs_rep_claim = np.empty(N_rep)
bs_rep_notclaim = np.empty(N_rep)
In [21]:
for i in range(10000):
    bs_rep_claim[i] = np.std(np.random.choice(claimed_insurance, size=len(claimed_insurance)))
    bs_rep_notclaim[i] = np.std(np.random.choice(notclaimed_insurance, size=len(notclaimed_insurance)))
bs_rep_diff = bs_rep_claim - bs_rep_notclaim
In [23]:
np.percentile(bs_rep_diff, [2.5, 97.5])
Out[23]:
array([6676.06177337, 8486.94648372])
In [31]:
plt.hist(bs_rep_diff)
plt.axvline(np.percentile(bs_rep_diff, 2.5), linestyle='-.', color='r')
plt.axvline(np.percentile(bs_rep_diff, 50), linestyle='-', color='y')
plt.axvline(np.percentile(bs_rep_diff, 97.5), linestyle='-.', color='g')
plt.xlabel('Diff in Std Deviations')
plt.ylabel('Frequency')
Out[31]:
Text(0, 0.5, 'Frequency')

Confidence interval and p-value
The confidence interval above is often a useful quantity to estimate. If we wish to limit our expected probability of making a Type I error (where we wrongly reject the null hypothesis, and it is, instead, true) to $\alpha$, the associated confidence interval is our estimate of the interval within which we expect the true population value to be found $100\times(1 - \alpha)$% of the time we do this test. In the above we performed bootstrap replicates to estimate the interval and reject the null hypothesis if this interval did not contain zero. You will sometimes see such an interval reported in the output of statistical functions.

The partner of the confidence interval is the p-value. The p-value and the confidence interval are linked through our choice of $\alpha$. The p-value tells us how likely it is, under the null hypothesis, to get an outcome at least as extreme as what was observed. If this fails to reach the level of our pre-specified $\alpha$, we decide the null hypothesis is sufficiently unlikely to be true and thus reject it. To calculate this p-value via the bootstrap, we have to put ourselves in a position where we are simulating the null hypothesis being true and then calculate the fraction of times we observe a result at least as extreme as that actually observed.

Remember how, previously, you used the t-test to calculate the p-value for the observed difference between the means of insured and non-insured medical cases. We're now going to repeat this, this time using the bootstrap approach.

Q: Perform a bootstrapped hypothesis test at the 5% significance level ($\alpha = 0.05$) to calculate the p-value of the observed difference between insurance and non-insurance charges, state your null and alternative hypotheses and whether you retain or reject the null hypothesis for the given significance level.

A:

In [59]:
claimed_mean = np.empty(N_rep)
not_claimed_mean = np.empty(N_rep)

for i in range(N_rep):
    bootstrap_sample =  np.random.choice( claimed_insurance, size=claimed_insurance.shape[0] )
    claimed_mean[i] = bootstrap_sample.mean()
    bootstrap_sample =  np.random.choice( notclaimed_insurance, size=notclaimed_insurance.shape[0] )
    not_claimed_mean[i] = bootstrap_sample.mean()

diff = claimed_mean - not_claimed_mean

lower_bound, upper_bound = tuple(np.percentile( diff, [2.5, 97.5] ))
In [60]:
print(f"The confidence interval is: {(lower_bound, upper_bound)}")
The confidence interval is: (6439.263104424963, 8735.576100039858)
In [ ]:

Q: To put the above result in perspective, plot the histogram of your bootstrapped differences along with lines marking the locations of the observed difference. (Why would we plot more than one line, given that we only have one observed difference?)

A:

In [61]:
plt.hist(diff, bins=50)
plt.axvline(diff.mean(),color='r')
plt.axvline(upper_bound,color='r',linestyle='--')
plt.axvline(lower_bound,color='r',linestyle='--')
plt.xlabel('Difference between mean of charges')
plt.ylabel('Number of People')
Out[61]:
Text(0, 0.5, 'Number of People')

Q: Compare your p-value above with that obtained using the t-test function in the previous assignment. Do you think you would want to try to perform enough bootstrap replicates to observe a random difference as large as that we did observe?

same

Q: Consider the two variants of the t-test we performed in the previous assignment. Which one would you use now?

A:

Q: If, instead of being asked whether the means of two groups were different, you were working with an ad-recommender team who wanted to release a new recommendation algorithm, and you were looking at click-through rate both for the current algorithm (call it A) and from trials of their new algorithm (call it B), would you perform a two-sided test as above? What would be your null and alternative hypotheses and what would be the real-world consequence of rejecting the null hypothesis?

A:

Learning outcomes
You've previously applied frequentist methods to calculate confidence intervals, p-values, and perform hypothesis tests. Frequentist methods use theoretical results to calculate what we expect would happen if experiments were to be run again and again and again. Now you've seen how you can do the same things using the bootstrap approach, which does not rely on such theory, and attendant assumptions, but instead literally does run experiments again and again and again.

In these exercises, you have:

calculated the same confidence interval lower limit as you did previously
tested the assumption that the variances of the two groups (insured vs. non-insured) were equal - something a bit harder to do using the frequentist method because of the nature of the sampling distribution for variance
calculated the p-value for the difference between the means of the two groups and compared with the result obtained using the previous frequentist approach
You are now well equipped to apply the bootstrap approach to a wide variety of problems. Just think about what conditions you wish to recreate in your simulated reruns.
